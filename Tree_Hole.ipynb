{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例：树洞软件中的NLP应用\n",
    "进入大三以来，压力比较大，因而接触到了一款树洞软件——“可话”，在这个软件的平台上我可以比较自在地分享一些事情。同时，“可话”的一大亮点是可以根据你的内容找到与你“共鸣”的话，你可以选择为其点赞和评论，从而交到志同道合的朋友。\n",
    "\n",
    "分析这款软件的背后逻辑，我们认为他有两大核心要点：\n",
    "1. 将“共鸣”这一应用抽象为**文本匹配**任务\n",
    "2. 通过用户行为可以提供标注数据，便于后续的模型调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 处理原始数据\n",
    "将数据集分为训练和测试的data，匹配的词典all_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>咱俩谁跟谁呀。</td>\n",
       "      <td>我们俩谁跟谁呀。</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>咱俩谁跟谁呀。</td>\n",
       "      <td>咱哥俩谁跟谁呀。</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>我拿了汪老师一本书。</td>\n",
       "      <td>我拿了汪老师的一本书。</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我给了她一只笔。</td>\n",
       "      <td>我拿了一支笔给她。</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>咱俩谁跟谁呀。</td>\n",
       "      <td>咱俩关系很好。</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12742</th>\n",
       "      <td>他没努力去解决这个问题，这个问题对他本来就很容易。</td>\n",
       "      <td>港大法律学院助理教授张达明也指，五区公投运动无涉及任何违反基本法的问题，只是一个政治行动。</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12743</th>\n",
       "      <td>他没努力去解决这个问题，这个问题对他本来就很容易。</td>\n",
       "      <td>这个方法以前曾暂时用于工业生产，但因价格昂贵而停止使用。</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12744</th>\n",
       "      <td>他没努力去解决这个问题，这个问题对他本来就很容易。</td>\n",
       "      <td>豆腐如果没有妥善保存或冰箱冷度不够，很容易馊掉，可以闻到或吃到酸味，此时不可再食用。</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12745</th>\n",
       "      <td>他没努力去解决这个问题，这个问题对他本来就很容易。</td>\n",
       "      <td>事实上，并不是每一具PSO-1瞄准镜都一定具有这个功能，目前在市场上有很多外型与PSO-1瞄...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12746</th>\n",
       "      <td>他没努力去解决这个问题，这个问题对他本来就很容易。</td>\n",
       "      <td>然而常见的复杂问题，支援可程式化片段的环境经常要辨别哪些是纯文字、哪些算作程式指令。</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12747 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           sent1  \\\n",
       "0                        咱俩谁跟谁呀。   \n",
       "1                        咱俩谁跟谁呀。   \n",
       "2                     我拿了汪老师一本书。   \n",
       "3                       我给了她一只笔。   \n",
       "4                        咱俩谁跟谁呀。   \n",
       "...                          ...   \n",
       "12742  他没努力去解决这个问题，这个问题对他本来就很容易。   \n",
       "12743  他没努力去解决这个问题，这个问题对他本来就很容易。   \n",
       "12744  他没努力去解决这个问题，这个问题对他本来就很容易。   \n",
       "12745  他没努力去解决这个问题，这个问题对他本来就很容易。   \n",
       "12746  他没努力去解决这个问题，这个问题对他本来就很容易。   \n",
       "\n",
       "                                                   sent2  label  \n",
       "0                                               我们俩谁跟谁呀。    1.0  \n",
       "1                                               咱哥俩谁跟谁呀。    1.0  \n",
       "2                                            我拿了汪老师的一本书。    1.0  \n",
       "3                                              我拿了一支笔给她。    1.0  \n",
       "4                                                咱俩关系很好。    1.0  \n",
       "...                                                  ...    ...  \n",
       "12742      港大法律学院助理教授张达明也指，五区公投运动无涉及任何违反基本法的问题，只是一个政治行动。   -1.0  \n",
       "12743                       这个方法以前曾暂时用于工业生产，但因价格昂贵而停止使用。   -1.0  \n",
       "12744         豆腐如果没有妥善保存或冰箱冷度不够，很容易馊掉，可以闻到或吃到酸味，此时不可再食用。   -1.0  \n",
       "12745  事实上，并不是每一具PSO-1瞄准镜都一定具有这个功能，目前在市场上有很多外型与PSO-1瞄...   -1.0  \n",
       "12746         然而常见的复杂问题，支援可程式化片段的环境经常要辨别哪些是纯文字、哪些算作程式指令。   -1.0  \n",
       "\n",
       "[12747 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'./dataset/'\n",
    "\n",
    "data = {\"sent1\":[],\"sent2\":[],\"label\":[]}\n",
    "with open(path+\"simtrain_to05sts.txt\",'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        s = line.replace('\\n','').split('\\t')\n",
    "        sent1,sent2,label = s[1],s[3],1.0 if eval(s[4])>2.5 else -1.0\n",
    "        data[\"sent1\"].append(sent1)\n",
    "        data[\"sent2\"].append(sent2)\n",
    "        data[\"label\"].append(label)\n",
    "data_df = pd.DataFrame(data,index=None)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv(path+\"data.csv\",encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>他走过去。他倒了一杯酒。他喝了一口。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我修电脑修了五百块钱。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>这本书不是我的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>他的主意没个更改。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>首先于一九四零年十月间重开浈缅路，接着派了一些在敦刻尔克撤退下来的残兵败将来中国学习游击战。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>我没看见你的钢笔。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>你穿太少了，当心着凉。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>翻跟头他不行。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>晒太阳的人们。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>差点儿答错。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sents\n",
       "0                                他走过去。他倒了一杯酒。他喝了一口。\n",
       "1                                       我修电脑修了五百块钱。\n",
       "2                                          这本书不是我的。\n",
       "3                                         他的主意没个更改。\n",
       "4    首先于一九四零年十月间重开浈缅路，接着派了一些在敦刻尔克撤退下来的残兵败将来中国学习游击战。\n",
       "..                                              ...\n",
       "995                                       我没看见你的钢笔。\n",
       "996                                     你穿太少了，当心着凉。\n",
       "997                                         翻跟头他不行。\n",
       "998                                         晒太阳的人们。\n",
       "999                                          差点儿答错。\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sents = {\"sents\":[]}\n",
    "with open(path+\"simtrain_to05sts.txt\",'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        s = line.replace('\\n','').split('\\t')\n",
    "        sent = s[1]\n",
    "        all_sents[\"sents\"].append(sent)\n",
    "all_sents['sents'] = list(set(all_sents['sents']))\n",
    "all_sents_df = pd.DataFrame(all_sents,index=None)\n",
    "all_sents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents_df.to_csv(path+\"all_sents.csv\",encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 分析数据集\n",
    "句子长度最长为196，最短为3，平均为24.5.我们选择150作为最大长度，可以看到有99.8的句子比这个小，所以是比较合适的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max = 196, min = 3, mean = 24.523260374990194\n"
     ]
    }
   ],
   "source": [
    "length_list = []\n",
    "for sent in list(data_df['sent1'])+list(data_df['sent2']):\n",
    "    length_list.append(len(sent))\n",
    "print(\"max = {}, min = {}, mean = {}\".format(np.max(length_list),np.min(length_list),np.mean(length_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9978426296383462\n"
     ]
    }
   ],
   "source": [
    "# 根据上述结果，我们尝试取150\n",
    "print(sum(1 if _ <= 150 else 0 for _ in length_list)/len(length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_max_len = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 将句子转换为字级别的词向量\n",
    "转换为字级别而没有转换为词组级别原因如下：\n",
    "字向量受到分词效果的影响较小，且embedding层的维数远远低于词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+\"data.csv\")\n",
    "\n",
    "# 标签及词汇表\n",
    "labels, evaluations = list(df['label'].unique()), list(df['sent1'].unique())+list(df['sent2'].unique())\n",
    "\n",
    "# 构造词组级别的特征\n",
    "vocabulary = []\n",
    "for evaluation in evaluations:\n",
    "    for word in evaluation:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary = set(vocabulary)\n",
    "\n",
    "# 字典列表\n",
    "word_dictionary = {word: i+1 for i, word in enumerate(vocabulary)}\n",
    "inverse_word_dictionary = {i+1: word for i, word in enumerate(vocabulary)}\n",
    "label_dictionary = {label: i for i, label in enumerate(labels)}\n",
    "output_dictionary = {i: labels for i, labels in enumerate(labels)}\n",
    "\n",
    "vocab_size = len(word_dictionary.keys()) # 词汇表大小\n",
    "label_size = len(label_dictionary.keys()) # 标签类别数量\n",
    "\n",
    "# 序列填充，按input_shape填充，长度不足的按0补充\n",
    "x1 = [[word_dictionary[word] for word in sent] for sent in df['sent1']]\n",
    "for i in range(len(x1)):\n",
    "    if len(x1[i])>sent_max_len:\n",
    "        x1[i] = x1[i][:sent_max_len]\n",
    "    else:\n",
    "        x1[i] = x1[i] + list(0 for j in range(sent_max_len-len(x1[i])))\n",
    "x2 = [[word_dictionary[word] for word in sent] for sent in df['sent2']]\n",
    "for i in range(len(x2)):\n",
    "    if len(x2[i])>sent_max_len:\n",
    "        x2[i] = x2[i][:sent_max_len]\n",
    "    else:\n",
    "        x2[i] = x2[i] + list(0 for j in range(sent_max_len-len(x2[i])))\n",
    "x = [np.array([x1[i],x2[i]]) for i in range(len(x1))]\n",
    "x = np.array(x)\n",
    "\n",
    "y = [_ for _ in df['label']]\n",
    "y = [np.array(label, dtype=np.float32) for label in y]\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型的训练\n",
    "本部分使用一个简单的DSSM_LSTM进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSSM_LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, dropout):\n",
    "        super(DSSM_LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(150*160, 50)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        a = self.embed(a).sum(1)\n",
    "        b = self.embed(b).sum(1)\n",
    "\n",
    "        a = self.lstm(a)[0]\n",
    "        b = self.lstm(b)[0]\n",
    "\n",
    "        a = self.dropout(a).reshape(-1, 150*160)\n",
    "        b = self.dropout(b).reshape(-1, 150*160)\n",
    "\n",
    "        a = self.fc(a)\n",
    "        b = self.fc(b)\n",
    "\n",
    "        cosine = torch.cosine_similarity(a, b, dim=1, eps=1e-8)   # 计算两个句子的余弦相似度\n",
    "\n",
    "        return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSSM_LSTM(\n",
       "  (embed): Embedding(4684, 100)\n",
       "  (lstm): LSTM(100, 80, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=24000, out_features=50, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用GPU加速\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "model = DSSM_LSTM(vocab_size=vocab_size+1, embedding_dim=100, hidden_size=80, dropout=0.2).cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(x)\n",
    "y = torch.from_numpy(y)\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.1, random_state = 0)\n",
    "train_x = train_x.unsqueeze(1)\n",
    "test_x = test_x.unsqueeze(1)\n",
    "train_x1 = train_x[:,:,0].cuda()\n",
    "train_x2 = train_x[:,:,1].cuda()\n",
    "test_x1 = test_x[:,:,0].cuda()\n",
    "test_x2 = test_x[:,:,1].cuda()\n",
    "train_y = train_y.cuda()\n",
    "test_y = test_y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置批大小\n",
    "BATCH_SIZE = 50\n",
    "# 设置损失函数\n",
    "loss_function = nn.MSELoss().cuda()\n",
    "# 设置优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.5577, train accuracy: 0.7212, test accuracy: 0.8227, time: 2022-01-02 21:05:09\n",
      "epoch: 1, train loss: 0.4681, train accuracy: 0.7961, test accuracy: 0.8494, time: 2022-01-02 21:05:16\n",
      "epoch: 2, train loss: 0.1503, train accuracy: 0.8855, test accuracy: 0.9020, time: 2022-01-02 21:05:23\n",
      "epoch: 3, train loss: 0.2429, train accuracy: 0.9056, test accuracy: 0.9145, time: 2022-01-02 21:05:30\n",
      "epoch: 4, train loss: 0.0374, train accuracy: 0.9374, test accuracy: 0.9514, time: 2022-01-02 21:05:37\n",
      "epoch: 5, train loss: 0.0890, train accuracy: 0.9557, test accuracy: 0.9514, time: 2022-01-02 21:05:44\n",
      "epoch: 6, train loss: 0.0195, train accuracy: 0.9643, test accuracy: 0.9545, time: 2022-01-02 21:05:51\n",
      "epoch: 7, train loss: 0.1328, train accuracy: 0.9680, test accuracy: 0.9576, time: 2022-01-02 21:05:58\n",
      "epoch: 8, train loss: 0.0141, train accuracy: 0.9714, test accuracy: 0.9537, time: 2022-01-02 21:06:04\n",
      "epoch: 9, train loss: 0.0196, train accuracy: 0.9727, test accuracy: 0.9624, time: 2022-01-02 21:06:12\n",
      "epoch: 10, train loss: 0.0961, train accuracy: 0.9731, test accuracy: 0.9482, time: 2022-01-02 21:06:19\n",
      "epoch: 11, train loss: 0.0011, train accuracy: 0.9743, test accuracy: 0.9553, time: 2022-01-02 21:06:26\n",
      "epoch: 12, train loss: 0.0808, train accuracy: 0.9749, test accuracy: 0.9498, time: 2022-01-02 21:06:33\n",
      "epoch: 13, train loss: 0.0758, train accuracy: 0.9752, test accuracy: 0.9537, time: 2022-01-02 21:06:40\n",
      "epoch: 14, train loss: 0.1626, train accuracy: 0.9761, test accuracy: 0.9380, time: 2022-01-02 21:06:47\n",
      "epoch: 15, train loss: 0.0023, train accuracy: 0.9779, test accuracy: 0.9631, time: 2022-01-02 21:06:54\n",
      "epoch: 16, train loss: 0.1819, train accuracy: 0.9762, test accuracy: 0.9584, time: 2022-01-02 21:07:01\n",
      "epoch: 17, train loss: 0.0870, train accuracy: 0.9790, test accuracy: 0.9569, time: 2022-01-02 21:07:08\n",
      "epoch: 18, train loss: 0.0786, train accuracy: 0.9812, test accuracy: 0.9624, time: 2022-01-02 21:07:15\n",
      "epoch: 19, train loss: 0.0314, train accuracy: 0.9824, test accuracy: 0.9678, time: 2022-01-02 21:07:22\n"
     ]
    }
   ],
   "source": [
    "train_N = train_x.shape[0]\n",
    "max_acc = 0.0\n",
    "# 训练\n",
    "for epoch in range(20):\n",
    "    batchindex = list(range(int(train_N / BATCH_SIZE))) # 按照批次大小，划分训练集合，得到对应批数据索引\n",
    "    random.shuffle(batchindex)   # 打乱索引值，便于训练\n",
    "    score_train = 0.0\n",
    "    for i in batchindex:\n",
    "        # 选取对应批次数据的输入和标签\n",
    "        batch_x1 = train_x1[i*BATCH_SIZE: (i+1)*BATCH_SIZE]\n",
    "        batch_x2 = train_x2[i*BATCH_SIZE: (i+1)*BATCH_SIZE]\n",
    "        batch_y = train_y[i*BATCH_SIZE: (i+1)*BATCH_SIZE]\n",
    "        # 模型预测\n",
    "        y_hat = model(batch_x1, batch_x2)\n",
    "        loss = loss_function(y_hat, batch_y)\n",
    "        optimizer.zero_grad()   # 梯度清零\n",
    "        loss.backward() # 计算梯度\n",
    "        optimizer.step()    # 更新参数\n",
    "        y_hat = torch.tensor([1.0 if _>=0.0 else -1.0 for _ in y_hat]).cuda()\n",
    "        score_train += torch.sum(y_hat == batch_y).float() / train_y.shape[0]\n",
    "\n",
    "    # test\n",
    "    y_hat = model(test_x1, test_x2)\n",
    "    y_hat = torch.tensor([1.0 if _>=0.0 else -1.0 for _ in y_hat]).cuda()\n",
    "    score = torch.sum(y_hat == test_y).float() / test_y.shape[0]\n",
    "    print(f\"epoch: {epoch}, train loss: {loss:.4f}, train accuracy: {score_train:.4f}, test accuracy: {score:.4f}, time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()) }\")\n",
    "\n",
    "    if score > max_acc:\n",
    "        torch.save(model, \"BEST_checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 用户行为的模拟阶段\n",
    "这一步实现两个任务：\n",
    "\n",
    "（1）用户输入一个句子，模型给出最佳匹配的6个句子（和软件中的设置一致）\n",
    "\n",
    "（2）用户根据自己的判断，选择是否要对给定的匹配“点亮”或“评论”。这里我们假定如果用户进行“点亮”或“评论”的行为，则将“label”设为“1”，不做任何操作则设为“-1”。那么，我们可以将（用户输入的句子，给定的匹配句子，label）这一三元组按顺序加载到数据集中，用做后续的模型微调（时间戳隐含于数据集的顺序中，微调时只需取出时间较近的句子）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSSM_LSTM(\n",
       "  (embed): Embedding(4684, 100)\n",
       "  (lstm): LSTM(100, 80, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=24000, out_features=50, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = torch.load(\"BEST_checkpoint.pt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入词典\n",
    "df = pd.read_csv(path+\"all_sents.csv\")\n",
    "# 序列填充，按input_shape填充，长度不足的按0补充\n",
    "x = [[word_dictionary[word] for word in sent] for sent in df['sents']]\n",
    "for i in range(len(x)):\n",
    "    if len(x[i])>sent_max_len:\n",
    "        x[i] = x[i][:sent_max_len]\n",
    "    else:\n",
    "        x[i] = x[i] + list(0 for j in range(sent_max_len-len(x[i])))\n",
    "x = [np.array(x[i]) for i in range(len(x))]\n",
    "x = np.array(x)\n",
    "len_x = len(x)\n",
    "x = torch.from_numpy(x)\n",
    "x = x.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户输入句子，并将句子做one-hot编码\n",
    "sent = \"屋里一个人也没有。\"\n",
    "y = sent\n",
    "y = [[word_dictionary[word] for word in y] for i in range(len_x)]\n",
    "for i in range(len_x):\n",
    "    if len(y[i])>sent_max_len:\n",
    "        y[i] = y[i][:sent_max_len]\n",
    "    else:\n",
    "        y[i] = y[i] + list(0 for j in range(sent_max_len-len(y[i])))\n",
    "y = np.array(y)\n",
    "y = torch.from_numpy(y)\n",
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行预测\n",
    "x = x.cuda()\n",
    "y = y.cuda()\n",
    "y_hat = list(model(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: 0: 那个瓜没十斤重。\n",
      "No: 1: 屋里连一个人也没有。\n",
      "No: 2: 人人都知道这件事。\n",
      "No: 3: 老雷一个字也没有透漏。\n",
      "No: 4: 他这个人就知道吃。\n",
      "No: 5: 一个人难免犯错误。\n"
     ]
    }
   ],
   "source": [
    "counts = {i: score for i, score in enumerate(y_hat)}\n",
    "items=sorted(list(counts.items()),key=lambda x:x[1],reverse=True)\n",
    "# 输出6条\n",
    "for i in range(6):\n",
    "    sent_ = df[\"sents\"][items[i][0]]\n",
    "    print(f\"No: {i}: {sent_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果看出，模型具有了一定的判断是否相似的能力，但是由于数据库过小（只有1000条），所以没有找到太好的匹配。这是很重要的一点问题，关系着是否能在上线后得到改进的问题（因为用户产生的新数据很可能都是负样本，样本过于不平衡的话，不利于模型的微调）\n",
    "\n",
    "这里我们假设用户对“屋里连一个人也没有。”和“一个人难免犯错误。”进行“点亮”或“评论”，则将六条新数据插入数据集data的最后，将用户的一条数据插入数据库词典all_sents中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入到data\n",
    "df_1 = pd.read_csv(path+\"data.csv\")\n",
    "df_2 = pd.read_csv(path+\"all_sents.csv\")\n",
    "sent1 = sent\n",
    "for i in range(6):\n",
    "    label = -1.0\n",
    "    if i == 2 or i == 5:\n",
    "        label = 1.0\n",
    "    sent2 = df_2[\"sents\"][items[i][0]]\n",
    "    df_1.loc[len(df_1)] = [sent1, sent2, label]\n",
    "df_1.to_csv(path+\"data.csv\", encoding='utf_8_sig',index=None)\n",
    "\n",
    "# 插入到all_sents\n",
    "df_2.loc[len(df_2)] = [sent1]\n",
    "df_2.to_csv(path+\"all_sents.csv\", encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 后续模型微调的模拟\n",
    "由于没有真实场景的用户反馈，所以只介绍思路：\n",
    "1. 导入保存的模型，设置为可训练的\n",
    "2. 用较新的数据进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "这次大作业中，选择了自己一个比较感兴趣的偏应用的nlp任务，在实践中也对树洞类软件进行了调研。未来如果有时间的话，希望自己能将其真正上线"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ab0d968002a578cf1e1aa041721a175249ba338f6da29efa5ef1a380c630376"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
